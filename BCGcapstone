#Initial Setup
from datetime import datetime as dt
import math
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import SGDClassifier, LogisticRegression, LogisticRegressionCV, LinearRegression
from statsmodels.formula.api import ols, glm
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix, 
                             classification_report, roc_curve, average_precision_score, f1_score, 
                             ConfusionMatrixDisplay)
from sklearn.model_selection import RandomizedSearchCV, cross_val_score

import xgboost as xgb
from xgboost import XGBClassifier, plot_importance

defaulter = pd.read_csv('data/credit card_defaults.csv')

pd.set_option('display.max_columns', None)
defaulter.head()

defaulter.info()

defaulter.isnull().sum()

defaulter.duplicated().sum()

#Exploratory Data Analysis
sns.countplot(x='SEX', data=defaulter)

sns.countplot(x='EDUCATION', data=defaulter)

sns.countplot(x='MARRIAGE', data=defaulter)

sns.countplot(x='default payment this month', data=defaulter)

sns.histplot(data=defaulter,x='AGE',bins=20)

class_0 = defaulter.loc[defaulter['default payment this month'] == 0]["LIMIT_BAL"]
class_1 = defaulter.loc[defaulter['default payment this month'] == 1]["LIMIT_BAL"]
sns.distplot(class_1,kde=True,bins=200, color="red")
sns.distplot(class_0,kde=True,bins=200, color="green")
plt.show()

def boxplot_variation(feature1, feature2, feature3, width=16):
    fig, ax1 = plt.subplots(ncols=1, figsize=(width,6))
    s = sns.boxplot(ax = ax1, x=feature1, y=feature2, hue=feature3,
                data=defaulter, palette="PRGn",showfliers=False)
    s.set_xticklabels(s.get_xticklabels(),rotation=90)
    plt.show();

boxplot_variation('AGE','LIMIT_BAL', 'default payment this month',16)

boxplot_variation('default payment this month','LIMIT_BAL', 'EDUCATION',12)

#Feature Engineering
defaulter_1 = defaulter.copy()
defaulter_1

#Latest Net Bill
defaulter_1['Net_Bill1'] = defaulter_1['BILL_AMT2'] - defaulter_1['PAY_AMT1']

#Outstanding balance counts and Days Past Due 90 days
OB_1 = []

for i in defaulter_1['BILL_AMT1']:
    if i <= 0 : OB_1.append(0)
    
    
    else: OB_1.append(1)
        
        
defaulter_1['OB_1'] = OB_1

OB_2 = []

for j in defaulter_1['BILL_AMT2']:
    if j <= 0 : OB_2.append(0)
    
    
    else: OB_2.append(1)
        
        
defaulter_1['OB_2'] = OB_2 

OB_3 = []

for k in defaulter_1['BILL_AMT3']:
    if k <= 0 : OB_3.append(0)
    
    
    else: OB_3.append(1)
        
        
defaulter_1['OB_3'] = OB_3

defaulter_1['Count_OB'] = defaulter_1['OB_1'] + defaulter_1['OB_2'] + defaulter_1['OB_3']

DPD_90days = []

for i in defaulter_1['Count_OB']:
    if i == 3: DPD_90days.append(1)
    
    else: DPD_90days.append(0)
        
        
defaulter_1['DPD_90days'] = DPD_90days

#Average bill & pay amount from past 6 months
defaulter_1['avg_bill'] = (defaulter_1['BILL_AMT1'] + defaulter_1['BILL_AMT2'] + defaulter_1['BILL_AMT3'] + defaulter_1['BILL_AMT4'] + defaulter_1['BILL_AMT5'] + defaulter_1['BILL_AMT6']) / 6
defaulter_1['avg_paid'] = (defaulter_1['PAY_AMT1'] + defaulter_1['PAY_AMT2'] + defaulter_1['PAY_AMT3'] + defaulter_1['PAY_AMT4'] + defaulter_1['PAY_AMT5'] + defaulter_1['PAY_AMT6']) / 6

#Net worth column based on percentile of credit limit balance for each ID
high_net_worth, affluent = defaulter_1.LIMIT_BAL.quantile(.75),defaulter_1.LIMIT_BAL.quantile(.25)
high_net_worth, affluent
defaulter_1['Net_worth'] = pd.cut(x = defaulter_1.LIMIT_BAL,bins = [0,affluent,high_net_worth,5555555], labels = [1, 2, 3], include_lowest=True)
defaulter_1.info()
defaulter_1['Net_worth'] = defaulter_1['Net_worth'].astype('int')
defaulter_1.info()
defaulter_1['Net_worth'].value_counts()
defaulter_1

#Train Test Split
x = defaulter_1.drop(columns=['default payment this month'])
y = defaulter_1['default payment this month']
vif = x.drop(['ID','BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6','OB_1','OB_2','OB_3','Count_OB','avg_bill','Net_worth'],axis=1)

vif['intercept'] = 1

vif_data = pd.DataFrame()
vif_data["feature"] = vif.columns

vif_data["VIF"] = [variance_inflation_factor(vif.values, i) for i in range(vif.shape[1])]
print(vif_data)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)

x_test_id = x_test['ID']
x_train = x_train.drop(columns=['ID'])
x_test = x_test.drop(columns=['ID'])

#SMOTE & Undersampling
pip install -U threadpoolctl

import imblearn
from collections import Counter
from imblearn.over_sampling import SMOTE
smt = SMOTE(random_state=42)
x_smt, y_smt = smt.fit_resample(x_train, y_train)
Counter(y_smt)

from imblearn.under_sampling import RandomUnderSampler
undersampler = RandomUnderSampler(random_state=42)
x_under, y_under = undersampler.fit_resample(x_train, y_train)
Counter(y_under)

#Model Training and Hyperparameter Tuning
#Model 1: Decision Tree
from sklearn.tree import DecisionTreeClassifier
dtree=DecisionTreeClassifier(criterion="entropy",random_state=100,max_depth=10,min_samples_leaf=5)
dtree.fit(x_train,y_train)
y_pred_dtree=dtree.predict(x_test)

display(ConfusionMatrixDisplay.from_predictions(y_test, y_pred_dtree))

print("DT_under accuracy score",round(accuracy_score(y_test, y_pred_dtree),2))
print(classification_report(y_test, y_pred_dtree))

import os
INSTALL_LOCATION = 'C:/Program Files/Graphviz/bin/'  # Edit this to match the install location on your computer
os.environ["PATH"] += os.pathsep + INSTALL_LOCATION

from six import StringIO  
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus

import graphviz
dot_data = StringIO()
export_graphviz(dtree, out_file=dot_data,
                feature_names=x_train.columns,
                filled=True, rounded=True,
                special_characters=True)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  

Image(graph.create_png())

feat_imp = {'Columns':x_train.columns.values,'Coefficients':dtree.feature_importances_}
feat_imp = pd.DataFrame()
feat_imp['Features'] = x_train.columns.values
feat_imp['importance'] = dtree.feature_importances_
feat_imp = feat_imp.sort_values(by='importance', ascending=False)
sns.barplot(y="Features", x="importance", data=feat_imp)

#Model 2: Decision Tree Undersampling
import time
dtree_plus = DecisionTreeClassifier(random_state=42)
from sklearn.model_selection import train_test_split, GridSearchCV
np.random.seed(42)
start = time.time()

param_dist = {'max_depth': [2, 3, 4],
              'max_features': ['auto', 'sqrt', 'log2', None],
              'criterion': ['gini', 'entropy']}

cv_dtree =GridSearchCV(dtree_plus, cv = 10,
                     param_grid=param_dist,
                     n_jobs = 3,scoring='accuracy')

cv_dtree.fit(x_under,y_under)
print('Best Parameters using grid search: \n',
     cv_dtree.best_params_)
end = time.time()
print('Time taken in grid search: {0: .2f}'.format(end - start))

DT_under=DecisionTreeClassifier(criterion="entropy",random_state=100,max_depth=4,min_samples_leaf=5)
DT_under.fit(x_under,y_under)
y_pred_DT_under=DT_under.predict(x_test)

display(ConfusionMatrixDisplay.from_predictions(y_test, y_pred_DT_under))

print("DT_under accuracy score",round(accuracy_score(y_test, y_pred_DT_under),2))
print(classification_report(y_test, y_pred_DT_under))

feat_imp = {'Columns':x_under.columns.values,'Coefficients':DT_under.feature_importances_}
feat_imp = pd.DataFrame()
feat_imp['Features'] = x_train.columns.values
feat_imp['importance'] = DT_under.feature_importances_
feat_imp = feat_imp.sort_values(by='importance', ascending=False)
sns.barplot(y="Features", x="importance", data=feat_imp)

#Model 3: Optimised Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=42)
from sklearn.model_selection import train_test_split, GridSearchCV
np.random.seed(42)
start = time.time()

param_dist = {'max_depth': [2, 3, 4, 5, 6, 7, 8],
              'max_features': ['auto', 'sqrt', 'log2', None],
              'criterion': ['gini', 'entropy']}

cv_rf_tree =GridSearchCV(rf, cv = 10,
                     param_grid=param_dist,
                     n_jobs = 3,scoring='accuracy')

cv_rf_tree.fit(x_train,y_train)
print('Best Parameters using grid search: \n',
     cv_rf_tree.best_params_)
end = time.time()
print('Time taken in grid search: {0: .2f}'.format(end - start))

rf.set_params(criterion='entropy', max_features= None, max_depth=8)

rf.fit(x_train,y_train)
y_pred_rf=rf.predict(x_test)

display(ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rf))

print("RF accuracy score",round(accuracy_score(y_test, y_pred_rf),2))
print(classification_report(y_test, y_pred_rf))

#feature Importance
feat_imp={'Columns':x_train.columns.values,'Coefficients':rf.feature_importances_}
feat_imp=pd.DataFrame()
feat_imp['Features']=x_train.columns.values
feat_imp['importance']=rf.feature_importances_
feat_imp=feat_imp.sort_values(by='importance', ascending=False)
sns.barplot(y="Features", x="importance", data=feat_imp)

#Model 4: Optimised XGBoost (Champion)
xgb_params = {'max_depth': [3, 5, 6, 8, 9, 10, 11], # Maximum depth of a tree
              'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5], # Step size shrinkage used in update to prevents overfitting
              'subsample': np.arange(0.4, 1.0, 0.1), #specifies the subsample ratio of the training instances that are randomly sampled without replacement to train each tree in the ensemble.
              'colsample_bytree': np.arange(0.3, 1.0, 0.1), # specifies the subsample ratio of the features that are randomly sampled without replacement to train each tree in the ensemble
              'colsample_bylevel': np.arange(0.3, 1.0, 0.1),# specifies the subsample ratio of the features that are randomly sampled without replacement to train each tree at each level. 
              'n_estimators': np.arange(100, 600, 100), #specifies the number of trees (estimators) to be used in the ensemble.
              'gamma': np.arange(0, 0.7, 0.1)} #controls the minimum reduction in the loss function required to make a split during tree building

# Loss function refers to the difference between the predicted y value with the actual y value. Eg. Mean Squared Error (MSE)

# Create RandomizedSearchCV instance

xgb_grid = RandomizedSearchCV(estimator=XGBClassifier(objective='binary:logistic', 
                                                      random_state=42),
                              param_distributions=xgb_params, 
                              cv=5, 
                              verbose=2,
                              n_iter=50,
                              scoring = recall_score
                             ) 

# cv specify the number of cross-validation folds to be used during the hyperparameter tuning process.
# verbose determines the verbosity level of the search process, meaning how much information about the progress and results of the search is printed to the console.
# verbose=0: No output will be printed during the search process.
# verbose=1: Some output will be printed, such as the progress of each iteration and the best parameters found so far.
# verbose=2: More detailed output will be printed, including the results of each iteration and the parameters tried.

# Run XGBoost grid search
xgb_grid.fit(x_train, y_train)

# Get best XGBoost model (based on best parameters) and predict on test set
xgb_best = xgb_grid.best_estimator_

xgb_grid.best_estimator_
y_pred_xgb = xgb_best.predict(x_test)
display(ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb))
print("XGB accuracy score",round(accuracy_score(y_test, y_pred_xgb),2))
print(classification_report(y_test, y_pred_xgb))
plot_importance(xgb_best)
plt.show()

x_train2 = x_train.drop(columns = ['Net_Bill1','EDUCATION', 'SEX', 'MARRIAGE','Net_worth','Count_OB','OB_1','DPD_90days','OB_2','OB_3'])
x_test2 = x_test.drop(columns = ['Net_Bill1','EDUCATION', 'SEX', 'MARRIAGE','Net_worth','Count_OB','OB_1','DPD_90days','OB_2','OB_3'])

xgb_params2 = {'max_depth': [3, 5, 6, 8, 9, 10, 11], # Maximum depth of a tree
              'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5], # Step size shrinkage used in update to prevents overfitting
              'subsample': np.arange(0.4, 1.0, 0.1), #specifies the subsample ratio of the training instances that are randomly sampled without replacement to train each tree in the ensemble.
              'colsample_bytree': np.arange(0.3, 1.0, 0.1), # specifies the subsample ratio of the features that are randomly sampled without replacement to train each tree in the ensemble
              'colsample_bylevel': np.arange(0.3, 1.0, 0.1),# specifies the subsample ratio of the features that are randomly sampled without replacement to train each tree at each level. 
              'n_estimators': np.arange(100, 600, 100), #specifies the number of trees (estimators) to be used in the ensemble.
              'gamma': np.arange(0, 0.7, 0.1)} #controls the minimum reduction in the loss function required to make a split during tree building

# Loss function refers to the difference between the predicted y value with the actual y value. Eg. Mean Squared Error (MSE)

# Create RandomizedSearchCV instance

xgb_grid2 = RandomizedSearchCV(estimator=XGBClassifier(objective='binary:logistic', 
                                                      random_state=42),
                              param_distributions=xgb_params2, 
                              cv=5, 
                              verbose=2,
                              n_iter=50,
                              scoring = recall_score
                             ) 

# cv specify the number of cross-validation folds to be used during the hyperparameter tuning process.
# verbose determines the verbosity level of the search process, meaning how much information about the progress and results of the search is printed to the console.
# verbose=0: No output will be printed during the search process.
# verbose=1: Some output will be printed, such as the progress of each iteration and the best parameters found so far.
# verbose=2: More detailed output will be printed, including the results of each iteration and the parameters tried.

# Run XGBoost grid search
xgb_grid2.fit(x_train2, y_train)

# Get best XGBoost model (based on best parameters) and predict on test set
xgb_drop = xgb_grid2.best_estimator_

xgb_grid2.best_estimator_

y_pred_xgb_drop = xgb_drop.predict(x_test2)
display(ConfusionMatrixDisplay.from_predictions(y_test, y_pred_xgb_drop))

print("XGB_drop accuracy score",round(accuracy_score(y_test, y_pred_xgb_drop),2))
print(classification_report(y_test, y_pred_xgb_drop))
plot_importance(xgb_drop)
plt.show()

#Identify Top 1000 Defaulters and its characteristics
preds_proba = xgb_drop.predict_proba(x_test2)
y_preds_proba = preds_proba[:,1]

cust_default_propensity = pd.DataFrame({'ID': x_test_id,
                                      'PROPENSITY': y_preds_proba})

cust_default_propensity.sort_values(by=['PROPENSITY'], ascending=False).head(10)

cust_default_propensity  = cust_default_propensity.merge(defaulter_1,
                                    on=['ID'],
                                    how='left')

cust_default_propensity['EXPECTED_LOSS'] = cust_default_propensity['PROPENSITY'] * cust_default_propensity['BILL_AMT1']
top_1000_loss = cust_default_propensity.sort_values(by=['EXPECTED_LOSS'], ascending=False).head(1000)
top_1000_loss.head()
top_1000_loss.describe()
top_1000_loss.SEX.value_counts()
top_1000_loss.EDUCATION.value_counts()
top_1000_loss.MARRIAGE.value_counts()
top_1000_loss.Net_worth.value_counts()

